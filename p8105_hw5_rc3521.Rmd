---
title: "p8105_hw5_rc3521"
author: "Runze Cui"
date: "2022-11-09"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
theme_set(theme_minimal() + theme(legend.position = "right"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1


Read and tidy data from the given longitudinal study

```{r, warning=FALSE}
files = list.files("./data", pattern = ".csv", all.files = FALSE, full.names = FALSE)

df = data.frame(participants = files) %>% 
  mutate(file_contents = purrr::map(participants, ~ read.csv(file.path("./data", .)))) %>% 
  separate(participants, into = c("control", "subject_id")) %>% 
  unnest(file_contents) %>% 
  mutate(
    control = recode(control, `con` = "control", `exp` = "experiment")
) 
```


Spaghetti plot
```{r}
df %>%  
  pivot_longer(week_1:week_8,
               names_to = "week",
               names_prefix = "week_",
               values_to = "observation") %>%
  ggplot(aes(x = week, y = observation, group = subject_id, color = subject_id)) +
  geom_path() + 
  facet_grid(~control) +
  labs(
    title = "Observations over time by groups",
    x = "Week",
    y = "Observation"
  ) + 
  viridis::scale_color_viridis(discrete = TRUE) + 
  theme(legend.position = "right")
```



## Problem 2

Read the homicides data:
```{r}
urlfile = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicides_df = read_csv(url(urlfile), na = c(" ", "Unknown")) 
```

Description: The `homicide` data contains 52,179 observations in 12 columns. Some variables such as `reported-date`, `victim_age` and `city` recorded really significant information on each observation of criminal homicides. 

Now, create new variables `city_state` and we can see the number of total murders and unsolved murders based on `city_state` variable.

```{r}
homicides_df =
  homicides_df %>%
  mutate(city_state = str_c(city, state, sep = ", "),
         resolution = case_when(
           disposition == "Closed without arrest" ~ "unsolved",
           disposition == "Open/No arrest" ~ "unsolved",
           disposition == "Closed by arrest" ~ "solved")) %>%
  relocate(city_state) %>% 
  filter(city_state != "Tulsa, AL")

homicides_df %>%
  group_by(city_state) %>%
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()) %>%
  knitr::kable(col.names = c("City", "Unsolved Murders", "Total Murders"))
```

Filter to Baltimore and `prop_test`

```{r}
baltimore_df =
  homicides_df %>%
  filter(city_state == "Baltimore, MD")

baltimore_summary =
  baltimore_df %>%
  summarise(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )

baltimore_test =
  prop.test(
  x = baltimore_summary %>% 
    pull(unsolved),
  n = baltimore_summary %>% 
    pull(n)
) %>% 
  broom::tidy()

baltimore_test
```


Make a iteration function for the to `prop_test` the whole `homicides_df` data frame

```{r}
prop_test_function =
  function(city_df) {
  
city_summary =
  city_df %>%
  summarise(
  unsolved = sum(resolution == "unsolved"),
  n = n()
  )

city_test = 
  prop.test(
  x = city_summary %>% 
    pull(unsolved),
  n = city_summary %>% 
    pull(n))
return(city_test)
}

prop_test_function(baltimore_df)

results =
  homicides_df %>%
  nest(data = uid:resolution) %>%
  mutate(
    test_results = map(data, prop_test_function),
    tidy_results = map(test_results, broom::tidy)
  ) %>%
  select(city_state, tidy_results) %>%
  unnest(tidy_results) %>%
  select(city_state, estimate, starts_with("conf"))

results
```

Make a plot of estimate and error bar for each city of our data frame.

```{r}
results %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    x = "City, State",
    y = "Proportion of Murders that are Unsolved",
    title = "Estinated Proportion of Murders which are Unsolved by City"
  )
```



## Problem 3

Do a hypothesis T test for $n = 30$ and $\sigma = 5$. The model follows normal distribution.

Set $\mu = 0$

Create a 5000 dataset for $\mu = 0$

```{r}
sim_mean_sd = function(n_obs, mu = 0, sigma = 5) {
  
  x = rnorm(n = n_obs, mean = mu, sd = sigma)
  t.test(x, n = n_obs, conf.level = 0.95)
  
}

output = vector("list", length = 5000)
for (i in 1:5000) {
  
  output[[i]] = sim_mean_sd(n_obs = 30) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
}

bind_rows(output)
```


Now we repeat above for $\mu = 1, 2, 3, 4, 5, 6$

```{r}
sim_results_df = 
  expand_grid(
    sample_size = 30,
    true_mu = c(0, 1, 2, 3, 4, 5, 6),
    iteration = 1:5000
  ) %>% 
  mutate(
    estimate_df = 
      map2(.x = sample_size, .y = true_mu, ~broom::tidy(sim_mean_sd(n_obs = .x, mu = .y)))
  ) %>% 
  unnest(estimate_df) %>% 
  select(true_mu, estimate, p.value) 

sim_results_stat = 
  sim_results_df %>% 
  group_by(true_mu) %>% 
  summarize(
    n = n(),
    n_reject = sum(p.value < 0.05),
    proportion = n_reject / n
  ) %>% 
  rbind()
sim_results_stat
```

Make a plot showing the proportion of times the null was rejected 

```{r}
sim_results_stat %>% 
  ggplot(aes(x = true_mu, y = proportion, color = true_mu)) + 
  geom_point() + 
  geom_line() + 
  labs(
    x = "True Mean",
    y = "proportion of rejecting null",
    title = "Proportion of Rejection VS True Mean"
  )
```


Based on the plot above, we can see that the proportion of rejecting null increases and gradually approaches one as the true mean grows, which means the power of the one-sample t-test increases as the true value of $\mu$ increases. Based on the knowledge, the statistical power will be associated with both effect size and sample size with a relatively trade-off relationship. 

```{r}
mu_ave = 
  sim_results_df %>% 
  group_by(true_mu) %>% 
  summarize(
    type = "average estimate mu",
    ave_est_mu = mean(estimate)
  )

rej_mu_ave = 
  sim_results_df %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mu) %>% 
  summarize(
    type = "average estimate mu in samples for which the null was rejected",
    ave_est_mu = mean(estimate)
  )

mu_ave %>% 
  rbind(rej_mu_ave) %>% 
  ggplot(aes(x = true_mu, y = ave_est_mu, group = type, color = type)) + 
  geom_point(aes(shape = type)) + 
  geom_line(aes(linetype = type)) + 
  labs(
    x = "True Mean",
    y = "Average Estimated Mean",
    title = "Average Estimated Mean VS True Mean"
  ) + 
  theme(legend.position = "bottom")
```

At the beginning of the plot, we clearly see that the sample average of estimated $\hat\mu$ across tests for which the null is rejected slightly differ to the true value of $\mu$ and tends to be equal to each other as true mean value increasing. But in general, we can say the sample average of estimated mu across tests for which the null is rejected is approximately equal to the true value of $\mu$. Since in this study, we generate 5,000 dataset for each value of $\mu$. The sample size is quite large and we can detect differences that are quite small and possibly trivial.




